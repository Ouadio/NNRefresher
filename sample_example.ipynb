{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import dense_layer as dl\n",
    "import dl_utils as dutils\n",
    "import importlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload modules for updates\n",
    "importlib.reload(dl)\n",
    "importlib.reload(dutils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Multi-linear Regression using NN\n",
    "Neural Networks' **representation capacity** is one of the key properties behind their great success *(Besides the optimizability of their underlying computational operations and the availability of great amounts of data etc..)*. \n",
    "But in order to go step-by-step, a fully-connected neural network reduced to one node-layer (perceptron) and emiting any non-linearity is equivalent to a linear regression model.  \n",
    "Thus, in this initial example, we define an arbitrary linear function $y = f_{lin}(X)$ where $X=\\{x1,x2,x3,x4\\} \\in R^4$ and $y \\in R$ by defining a set of coefficients $c_i, \\ i \\in [1:4]$ and an intercept $c_0$.\n",
    "\n",
    "$f_{lin} : R^4 -> R$ \n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ X -> c_0 + \\Sigma_{i=1}^4c_ix_i$\n",
    "\n",
    "We randomly generate a set of $m$ observations following $f_{lin}(X) | X$ joint distrubution. \n",
    "This simple notebook demonstrates how we can fit such continuous function *(=> regression problem)* using a simple Perceptron (Neural network with one layer & node), incorporating a multi-linear regression *(with an input of 4 and a non-null intercept in this case)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size (Number of observations)\n",
    "m = 4096\n",
    "\n",
    "# Input features (Independent variables)\n",
    "x1 = np.random.uniform(low=-10, high=20, size=[m])\n",
    "x2 = np.random.uniform(low=0, high=10, size=[m])\n",
    "x3 = np.random.normal(loc=5, scale=10, size=[m])\n",
    "x4 = np.random.normal(loc=-1, scale=3, size=[m])\n",
    "\n",
    "# Coefs & intercept\n",
    "coefs_lin = np.array([2, -0.5, 3.5, -1.1])\n",
    "intercept_lin = 6.8\n",
    "\n",
    "\n",
    "def f_lin(x_1, x_2, x_3, x_4):\n",
    "    return coefs_lin[0] * x_1 + coefs_lin[1] * x_2 + coefs_lin[2] * x_3 + coefs_lin[3] * x_4 + intercept_lin\n",
    "\n",
    "\n",
    "# Label (y=f(x) + noise) (Dependent variable)\n",
    "y = f_lin(x1, x2, x3, x4)\n",
    "y += np.random.randn(m) * np.std(y)/10\n",
    "X = np.array([x1, x2, x3, x4]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test splitting\n",
    "X_train, X_test, y_train, y_test = dutils.train_test_split(\n",
    "    X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Linear regression model (1 Layer only (Output layer) + Indentity activation)\n",
    "MultiLinRegNN = dl.Sequential(\n",
    "    [dl.Dense(out_size=1, in_size=4, activation=\"identity\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "MultiLinRegNN.compile(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights ~ Coefficients\n",
    "# Bias ~ Intercept\n",
    "print(f\"Weights before :\\n {MultiLinRegNN._layers[0]._W}\")\n",
    "print(f\"Bias before : {MultiLinRegNN._layers[0]._b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model's accuracy (using loss function. TODO : Metric can be different)\n",
    "MultiLinRegNN.evaluate(X_data=X_test, y_label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model & return its loss function training history log\n",
    "lossHist = MultiLinRegNN.train(X_data=X_train, y_label=y_train,\n",
    "                               minibatch=64, shuffle=True,\n",
    "                               n_iterations=100, lr=0.02, min_loss=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model's accuracy after training (on training data)\n",
    "MultiLinRegNN.evaluate(X_data=X_train, y_label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model's accuracy after training (on test data)\n",
    "MultiLinRegNN.evaluate(X_data=X_test, y_label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's weights after training\n",
    "MultiLinRegNN._layers[0]._W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling model's weights to account for the internal data std normalization\n",
    "# applied during training\n",
    "scaled_W = np.divide(MultiLinRegNN._layers[0]._W.reshape(\n",
    "    1, -1), MultiLinRegNN._in_features_std)\n",
    "scaled_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learned weights against true coefficients :\n",
    "np.abs(coefs_lin - scaled_W) / (np.abs(coefs_lin + scaled_W)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's bias after training\n",
    "MultiLinRegNN._layers[0]._b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling & shifting model's bias to account for the internal data std and mean\n",
    "# normalizations applied during training\n",
    "scaled_b = MultiLinRegNN._layers[0]._b - \\\n",
    "    np.sum(np.multiply(scaled_W, MultiLinRegNN._in_features_mean))\n",
    "scaled_b = scaled_b[0]\n",
    "scaled_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learned bias against true intercept :\n",
    "np.abs(intercept_lin - scaled_b) / (np.abs(intercept_lin + scaled_b)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent loss function evolution during training\n",
    "plt.scatter(x=np.arange(0, lossHist.shape[0]), y=lossHist, c=\"black\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
