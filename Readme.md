# Fully Connected Neural Network Petite Library

This is a basic Fully Connected Neural Network python based library, a minimalisitic set of modules designed from the ground up to provide a straightforward and efficient way to build, train, and infer fully connected neural networks. This library-project is developed with the aim to serve as an educational reference using documented and simple python programming tools & techniques, while remaining flexible and ready for plug-and-play for further experimentations and extensions.

## Features developed so-far

- **Customizable Network Architecture**: Define your neural network with any number of layers and neurons to fit your specific problem.
- **Flexible Activation Functions**: Comes with popular activation functions like Sigmoid, ReLU, and Tanh, and allows for easy integration of custom functions.
- **Base-line Training Algorithms**: Implements gradient descent *(and derivatives such as mini-batch gradient descent)* and backpropagation algorithms.
- **Data Preprocessing Tools**: Includes functionalities to normalize and preprocess your data for optimal training results.
- **Model Evaluation Metrics**: Provides a suite of evaluation metrics for analyzing the performance of your neural network.

Besides, a well documented jupyter-notebook is available for demonstration purposes and can thus serve as a starting point for exploring the modules.


Reader might notice two things : 
- Some non-pythonic syntaxic/programming styles, which can be traced back to my 'learned' C++ programming habits.
- Some Keras-like *(The DL framework on top of TensorFlow)* notations and conventions, which actually reflect my past experiences in ML/DL using the Keras framework *(way before Pytorch became a big thing)*.

*Note*:
This project was also an opportunity for me to play a bit *again* with Python programming language *(having spent most of the last 3 years working with C++)*. Embarking on this simple project has thus served as an invaluable refresher and a hands-on rehearsal of the fundamental concepts that underpin the ML/DL fields. And last but not least, eventhough the DL field is nowadays well established and there are numerous highly optimized, scalable and cross-platform frameworks and toolkits, I prefer spending more time than usual trying to replicate their functionalities rather than jump to theiur usage straight-way. I deeply believe in Richard Feynmann's unfamous quote "What I cannot create, I do not understand". 
